# -*- coding: utf-8 -*-
"""Assignment#3-ML_Ruben_Kritika.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S87xWd3r6vnV0AelTgPNyA6IzWKcx94z

# Assignment #3 - Machine Learning

1. Run Random Forest
2. Return Importance Feature Rank
3. Filter Data based on Importance
4. Perform Cross Validation Based on Training Data (KNN, LDA, Random Forst)
5. Generate AUPRC and ROC Curves
6. Predict Class Probability for Test Instances
"""

# Libraries
import sys
import pandas as pd
import numpy  as np 
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
from sklearn.utils.fixes import signature
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
from sklearn.utils.fixes import signature
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc
# from scipy.interpolate import UnivariateSpline
import warnings
warnings.simplefilter(action='ignore')

def readData(train_filename, test_filename):
  # Read Data
  train_df = pd.read_csv(train_filename, sep='\t', header=None)
  test_df  = pd.read_csv(test_filename, sep='\t', header=None)
  # Shuffle Rows
  train_df = train_df.sample(frac=1)  # frac specifies the percentage of entire dataset to return. 1 = 100% = All
  test_df  = test_df.sample(frac=1)
  # Separate label from training data
  train_y = train_df.values[:,  -1]
  train_x = train_df.values[:, :-1]
  return train_df, test_df, train_x, train_y

"""# Filter By Importance"""

def filterData(train_x, train_y, test_x):
  # Run Random Forest to get the Importance Rank
  forest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)
  forest.fit(train_x, train_y)
  importances = forest.feature_importances_
  std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)
  indices = np.argsort(importances)[::-1]
  # Plot the feature importances of the forest
  plt.figure(figsize=(5,11))
  plt.title("Feature importances")
  plt.barh(range(train_x.shape[1]), importances[indices],
        color="r", align="center")
  plt.yticks(range(train_x.shape[1]), indices)
  plt.ylim([-1, train_x.shape[1]])
  plt.ylabel("Features")
  plt.xlabel("Importance")
  # Filter by importance
  importance_threshold = 0.00035
  train_x = train_x[: , importances > importance_threshold ]
  test_x  = test_x[ : , importances > importance_threshold ]
  print("Train Data Reduced to {} Based on Importance".format(train_x.shape ))
  plt.draw()
  plt.pause(0.01)
  plt.pause(0.01)
  return train_x, test_x

"""# Base ML Model"""

class MLModel():
  def __init__(self, model=None, cv=10, param_grid=[], title="Model"):
    self.f, (self.ax1, self.ax2) = plt.subplots(1, 2, sharey=True)
    self.f.suptitle(title)
    self.ax1.set_title('ROC Curve Per Fold')
    self.ax2.set_title('AUPRC Curve Per Fold')
    self.title = title
    #General GridSearchCV waiting for the special parameters for each ML model
    self.model = self.model  = GridSearchCV(
        model, 
        param_grid, 
        cv      = cv, 
        iid     = False,
        scoring = ['average_precision','precision', 'recall'],
        refit   = 'average_precision',
    )
  
  def train(self, X, y):
    print("TRAIN MODEL: ", self.title)
    self.model.fit(X,y)
  
  def drawCurves(self, X, y, n_split=10):
    print("DRAW CURVES: ", self.title)
    ave_precision = []
    ave_recall    = []
    ave_fpr       = []
    ave_tpr       = []
    ave_pre_score = []
    ave_roc_auc   = []
    # Stratified KFold to distribute positives and negatives equally in each fold
    skf = StratifiedKFold(n_splits=n_split)
    skf.get_n_splits(X, y)
    # The algorithms sklearn roc and auprc curve algoritms output 
    # precision, recall, fpr, tpr with different shapes
    # To calculate the average of all we create a bi-dimensional matrix of each
    # With the size of the row of the minimum array output by the methods of sklearn
    min_pre_row_size = 999
    min_rec_row_size = 999
    min_fpr_row_size = 999
    min_tpr_row_size = 999
    for train_index, test_index in skf.split(X, y):
      _, X_test = X[train_index], X[test_index]
      _, y_test = y[train_index], y[test_index]

      # Obtain Prediction
      predict_proba        = self.model.predict_proba(X_test)
      y_score              = predict_proba[:, 1]                 #Pos 1 gives probability of being positive( 1 )
      average_precision    = average_precision_score(y_test, y_score)
      # Obtain Performance Metrics
      precision, recall, _ = precision_recall_curve(y_test, y_score)
      fpr, tpr, _          = roc_curve(y_test,  y_score)
      roc_auc              = roc_auc_score(y_test,  y_score)
      # Save to memory
      ave_precision.append(precision)
      ave_recall.append(recall)
      ave_fpr.append(fpr)
      ave_tpr.append(tpr)
      ave_pre_score.append(average_precision)
      ave_roc_auc.append(roc_auc)
      # Get the minimum size of array per each metric
      if min_pre_row_size > len(precision):
        min_pre_row_size = len(precision)
      if min_rec_row_size > len(recall):
        min_rec_row_size = len(recall)
      if min_fpr_row_size > len(fpr):
        min_fpr_row_size = len(fpr)
      if min_tpr_row_size > len(tpr):
        min_tpr_row_size = len(tpr)
    # Calculate Average Performance for each metric and plot performance per fold and 
    # Plot Average Perfomance
    np_ave_pre = np.array([]).reshape(0, min_pre_row_size)
    np_ave_rec = np.array([]).reshape(0, min_rec_row_size)
    np_ave_fpr = np.array([]).reshape(0, min_fpr_row_size)
    np_ave_tpr = np.array([]).reshape(0, min_tpr_row_size)
    for i in range(10):
      self.ax1.step(ave_fpr[i]   ,ave_tpr[i]       , alpha=0.1)
      self.ax2.step(ave_recall[i], ave_precision[i], alpha=0.1)
      np_ave_pre = np.vstack( [np_ave_pre, ave_precision[i][:min_pre_row_size]] )
      np_ave_rec = np.vstack( [np_ave_rec, ave_recall[i][:min_rec_row_size]] )
      np_ave_fpr = np.vstack( [np_ave_fpr, ave_fpr[i][:min_fpr_row_size]] )
      np_ave_tpr = np.vstack( [np_ave_tpr, ave_tpr[i][:min_tpr_row_size]] )
    mean_tpr     = np_ave_tpr.mean(0)
    mean_fpr     = np_ave_fpr.mean(0) 
    
    self.ax1.step(mean_fpr          , mean_tpr          , alpha=1.0, label="Fold #{}".format(i) )
    self.ax2.step(np_ave_rec.mean(0), np_ave_pre.mean(0), alpha=1.0, label="Fold #{}".format(i) )
    self.ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)
    # Output Plot. Two pause necessary for renderer to finish. 1 alone doesn't work
    self.f.suptitle(self.title+"Params: "+ str(self.model.best_params_ ))
    print(len(ave_roc_auc))
    self.ax1.set_title('ROC Curve. AUC: {:.2f}'.format(np.mean(ave_roc_auc))) 
    self.ax2.set_title('AUPRC AP: {:.2f}'.format(np.mean(ave_pre_score)))
    plt.draw()
    plt.pause(0.01)
    plt.pause(0.01)
    return np.mean(ave_pre_score)
    
  def test(self, X):
    predict_proba        = self.model.predict_proba(X)
    print("Results can be seen in the results.txt file")
    for p in predict_proba:
      # print(p[1])
      with open('results.txt','a') as fd:
        fd.write(str(p[1])+"\n")
    return predict_proba[:, 1]

"""# KNN"""

class KNN(MLModel):
  def __init__(self, k_min=3, k_max=41, ):
    self.k_range = list(range(k_min,k_max))
    self.param_grid = dict(n_neighbors = self.k_range)
    MLModel.__init__(
        self, 
        model=KNeighborsClassifier(), 
        param_grid=self.param_grid, 
        title="KNN")

"""# Random Forest"""

class RandomForest(MLModel):
  def __init__(self):
    self.param_grid = {  'max_features': [None, "sqrt", "log2" ] }
    MLModel.__init__(
        self, 
        model=RandomForestClassifier(min_samples_leaf=5, n_estimators=500), 
        param_grid=self.param_grid, 
        title="Random Forest")

"""# Logistic Regression"""

class CustomLogisticRegression(MLModel):
  def __init__(self ):
    self.param_grid = {"C":np.logspace(-3,3,7), "penalty":["l1","l2"]}
    MLModel.__init__(
        self, 
        model=LogisticRegression(), 
        param_grid=self.param_grid, 
        title="Logistic Regression")

"""# Predict"""
if __name__ == '__main__':
  # Arguments
  train_filename = sys.argv[1]
  test_filename  = sys.argv[2]

  results       = {}
  algorithms    = {}

  train_df,       \
  test_df,        \
  train_x, train_y = readData(train_filename, test_filename)
  train_x, test_x = filterData(train_x, train_y, test_df.values)

  train_df.head()

  test_df.head()

  knn = KNN()
  knn.train(train_x, train_y)
  results["KNN"] = knn.drawCurves(X=train_x, y=train_y)
  algorithms["KNN"] = knn

  rf = RandomForest()
  rf.train(train_x, train_y)
  results["RANDOM FOREST"] = rf.drawCurves(X=train_x, y=train_y)
  algorithms["RANDOM FOREST"] = rf

  lr = CustomLogisticRegression()
  lr.train(train_x, train_y)
  results["LOGISTIC REGRESSION"] = lr.drawCurves(X=train_x, y=train_y)
  algorithms["LOGISTIC REGRESSION"] = lr

  bestModelKey = max(results, key=results.get)
  print("Best Model: " ,bestModelKey, "\n\n")
  algorithms[bestModelKey].test(test_x)
  plt.show()