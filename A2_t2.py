# -*- coding: utf-8 -*-
"""Assignment #2 - Ruben

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y8zfz3esy0x5Pf4KZkQZxHdgZz6-GqdE

# Part 2

# Hyper Parameters and Libraries
"""

#Console Parameters
import sys 
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

#Machine Learning
from sklearn.neighbors import KNeighborsClassifier
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
from sklearn.metrics import explained_variance_score
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import accuracy_score,  precision_recall_curve, average_precision_score, mean_squared_error
from sklearn.datasets import make_classification
from sklearn.metrics import precision_recall_curve
from sklearn.utils.fixes import signature
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
  
#Utils - Math and Plotting
import matplotlib.pyplot as plt
from random import randrange
import pandas as pd
import numpy as np
import random

#Hyperparameters
debug               = False
# varianceThreshold   = 0.0005
varianceThreshold   = 0.001
K                   = 17
FoldSize            = 10
PossibleModelsCSV   = "PossibleModelsTable.csv"
isNotebook          = True

def loadDataset(filename):
  print("Loading Dataset (Process 1 of 7)")
  df = pd.read_csv( filename ,sep='\t', header=None)
  data = np.array( df.as_matrix() )
  if debug:
    print(df.head())
    print("\nRaw_Data_Shape\t{}\tNumber_of_Features\t{}".format(df.shape, len(df)) )
  return data

"""# Order By Variance"""

def OrderByVariance(data):
  print("Order and Filter By Variance (Process 2 of 7)")
  #Take Out Labels from data
  features_without_label = data[: , :-1]
  #Separate Labels in a different array
  labels                 = data[: , -1]
  #Get Variance per Column
  variance               = np.var(features_without_label,0)
  #Sort Variance per Column From Bigger To Smaller
  new_index_by_var_sort  = np.argsort(variance)[::-1]                
  selector               = VarianceThreshold(varianceThreshold) 
  #Filter Columns, Delete all columns with variance lesser than threshhold
  data                   = selector.fit_transform(data[:,new_index_by_var_sort])
  #append again the labels
  data                   = np.c_[data, labels ]
  if debug:
    print("\n\nfeatures_without_label\t{}\tlabels\t{}".format(features_without_label.shape, labels.shape))
    print("\n\nTable of Variance Per Feature\n\n",       pd.DataFrame([variance], index=["Variance"]))
    print("\nNew Order Based on Features with Higher Variance to lower\n",
    pd.DataFrame([
        variance[new_index_by_var_sort]
    ], columns=new_index_by_var_sort,
    index=["Variance"]))
    print("\nVariance Filtered Data (with Label Col.) \n", pd.DataFrame(data).head())
  return data

"""# Cross Validation Split"""

def CrossValidationSplit(dataset, folds=10):
  # Divide the dataset D pseudo-randomly into V folds
  dataset_split = list()
  dataset_copy = list(dataset)
  fold_size = int(len(dataset) / folds)
  for i in range(folds):
    fold = list()
    while len(fold) < fold_size:
      index = randrange(len(dataset_copy))
      fold.append(dataset_copy.pop(index))
    dataset_split.append(fold)
  return dataset_split

def EqualyDistributePositiveAndNegativeAndSplit(data):
  print("Cross Validation Split (Process 3 of 7)")
  #Separate Positives & Negatives
  p_indexes      = (data[:,-1] == 1 )
  n_indexes      = (data[:,-1] == 0 )
  data_positives = data[p_indexes , :]
  data_negatives = data[n_indexes,  :]
  #Cross Validation to positives and negatives
  p_folds         = np.array(CrossValidationSplit(data_positives, FoldSize))
  n_folds         = np.array(CrossValidationSplit(data_negatives, FoldSize))
  n_rows_per_fold = p_folds.shape[1] + n_folds.shape[1]
  #Empty Array To Store Folds
  folds = np.array([]).reshape(0,n_rows_per_fold, data.shape[1])
  #Join in equaly distributed way, positives and negatives
  for i in range(FoldSize):
    f = np.r_[p_folds[i] , n_folds[i] ] 
    #Shuffle features
    np.random.shuffle(f)
    folds = np.append( folds ,[f], axis=0 )
  if debug:
    print("\nPositives\t{}".format(    data_positives.shape) )
    print("Negatives\t{}".format(data_negatives.shape))
    print("Folds_Positives\t{}\nFolds_Negatives\t{}\nFolds_Shuffled_and_Joined\t{}".format(p_folds.shape, n_folds.shape, folds.shape))
  return folds, n_rows_per_fold

"""#Cross Validation - Choose the Best Model"""

def OneDArrayToKey(array):
  key = ','.join(str(a) for a in array)
  return key

def keyToOneDArray(key):
  array = np.array([int(a) for a in key.split(',')]) 
  return array


# x  = np.array([0, 1, 2, 3])
# s  = OneDArrayToKey(x)
# print(s)
# aa = keyToOneDArray(s)
# print(aa)

def CrossValidation(folds, data, K):
  bestModels =  {k: {} for k in range(3, K, 2)  }
  print("Cross Validation-Selecting Best Model (Process 4 of 7)")
  if(debug):
    print("\n\nStarting Cross Validation\n\n")
  for i, f in enumerate(folds): 
    dataset_copy         =  list(folds)
    #Define set T as the I-th fold of the dataset D
    test_fold            =  dataset_copy.pop(i)  
    #Define set L as the dataset D without the I-th fold
    dataset_without_fold =  dataset_copy       
    dataset_without_fold =  np.array(dataset_without_fold).reshape((-1, data.shape[1]))
    bestValues  = {
        "fSelectedIndexes": np.array([0], dtype=int),  
        "auc"       :       0.0
    }
    #Iterate through all the features to select only the bests
    for feature_index in range(1, dataset_without_fold.shape[1] - 1) : 
      #Exclude the last column because is labels
      #Append the next feature index to be tested
      bestEpisodeValues = {
        "fSelectedIndexes": np.append( bestValues["fSelectedIndexes"], feature_index ),   
        "auc"       :       0.0
      }
      #Create Dataset Only With The Testing Feature Indexes
      X = dataset_without_fold[:, bestEpisodeValues["fSelectedIndexes"] ]
      y = dataset_without_fold[:, -1].reshape(-1)
      test_data       = test_fold[:, bestEpisodeValues["fSelectedIndexes"] ]
      test_true_label = test_fold[:, -1].reshape(-1)
      #Iterate through all the possible K values in every Feature Test and every Fold Test
      for k in range(3, K, 2):
        clf                           = KNeighborsClassifier(n_neighbors=k)
        training_acc                  = clf.fit(X, y).score(X, y)
        # predictions                   = clf.predict(test_data)
        predictions                   = clf.predict(test_data)
        predictions_prob              = clf.predict_proba(test_data)[:, 1] #Probability of being 1
        fpr, tpr, roc_thresholds      = roc_curve( test_true_label, predictions_prob)
        roc_auc                       = auc(fpr, tpr)
        featuresKey                   = OneDArrayToKey(bestEpisodeValues["fSelectedIndexes"])
        selectedKDict                 = bestModels[k]
        if featuresKey in selectedKDict:
          selectedKDict[featuresKey].append(roc_auc)
        else:
          selectedKDict[featuresKey] = [roc_auc]
        
        #If the accuracy for this model is better than the previous, replace
        if( roc_auc > bestEpisodeValues["auc"] ):
          bestEpisodeValues["auc"]              = roc_auc 
          bestEpisodeValues["featureIndex"]     = feature_index 
      #If the model selected for this testing set of features is better than the previous
      #Replace K value and append new feature index, in the list of best features
      if( bestEpisodeValues["auc"] > bestValues["auc"] ):
        if bestEpisodeValues["featureIndex"] not in bestValues["fSelectedIndexes"]:
          bestValues["fSelectedIndexes"] =   np.append(bestValues["fSelectedIndexes"], bestEpisodeValues["featureIndex"] ) 
        bestValues["auc"]              = bestEpisodeValues["auc"] 
  
  #Convert Hash Table of Possible K vs Features vs AUC Values Per Fold
  #2D Matrix of Possible K vs Features with average AUC
  k_indexes      = range(3, K, 2)
  BestModelTable = []
  for k, features in bestModels.items():
    aucPerFeature = []
    f_indexes     = []
    for f, auc_values in features.items():
      f_indexes.append(f)
      aucPerFeature.append(np.mean(auc_values))
    BestModelTable.append(aucPerFeature)

  #Save Possible Model Table to CSV  
  BestModelTableDF = pd.DataFrame(BestModelTable, index=k_indexes, columns=f_indexes)
  i,j = np.unravel_index(np.array(BestModelTable).argmax(), np.array(BestModelTable).shape)
  if debug:
    print('\n\nBest Value AUC ', BestModelTable[i][j], ' k ',k_indexes[i], 'feature' , keyToOneDArray(f_indexes[j])   )
  #Save All Possible Model Tables  
  BestModelTableDF.to_csv(PossibleModelsCSV, sep='\t', index=True)
    
#   return bestValues  
  return {'auc_score': BestModelTable[i][j], 'k': k_indexes[i], 'features': keyToOneDArray(f_indexes[j]) } , BestModelTable, f_indexes

"""# Plot Performance Metrics"""

def PlotCurves(folds, bestModel, data ):
  print("Ploting Precision-Recall & ROC Curve (Process 6 of 7)")
  #GraphPointers
  pre_rec_fig = plt.figure()
  roc_fig     = plt.figure()
  pre_rec_ax = pre_rec_fig.gca()
  roc_ax     = roc_fig.gca()
  #Iterate all Folds
  for i, f in enumerate(folds):
    #Prepare Data as in CV function, but this time with the Selected Best Features
    dataset_copy         =  list(folds)
    test_fold            =  dataset_copy.pop(i)  
    dataset_without_fold =  dataset_copy         
    dataset_without_fold =  np.array(dataset_without_fold).reshape((-1, data.shape[1]))
    X                    = dataset_without_fold[:, bestModel["features"] ]
    y                    = dataset_without_fold[:, -1].reshape(-1)
    test_data            = test_fold[:, bestModel["features"] ]
    test_true_label      = test_fold[:, -1].reshape(-1)
    #Train Per Fold with Best Features to get Average Scores
    clf                           = KNeighborsClassifier(n_neighbors=bestModel["k"])
    clf.fit(X, y) 
    predictions                   = clf.predict(test_data)
    predictions_proba             = clf.predict_proba(test_data)[:, 1] #Probability of being 1
    #Scores
    precision, recall, thresholds = precision_recall_curve( test_true_label, predictions_proba)
    fpr, tpr, roc_thresholds      = roc_curve( test_true_label, predictions_proba)
    roc_auc                       = auc(fpr, tpr)
    ave_precision                 = average_precision_score(test_true_label, predictions)
    #Plots Per Folds
    pre_rec_ax.plot(precision, recall, lw=1, alpha=0.3,
              label='Precision-Recall fold %d (Ave. Precision = %0.2f)' % (i, ave_precision))
    roc_ax.plot(fpr, tpr, lw=1, alpha=0.3,
              label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))
    if(debug):
      print("Fold#{}\tauc\t{:5f}\tprecision\t{}".format(i, roc_auc, ave_precision))
  pre_rec_fig.legend(loc="upper right")
  pre_rec_ax.set_xlabel('Recall')
  pre_rec_ax.set_ylabel('Precision')
  pre_rec_ax.set_title('Precision-Recall Curve')
  roc_fig.legend(loc="lower right")
  roc_ax.set_xlabel('False Positive Rate')
  roc_ax.set_ylabel('True Positive Rate')
  roc_ax.set_title('ROC Curve')

def PlotKMapMetrics(folds, bestModel, BestModelTable, f_indexes, data):
  print("Rendering Best Model Features VS All Posible K (Process 7 of 7)")
  k_fig = plt.figure()
  k_ax     = k_fig.gca()
  #Data Preparation
  k_range = range(3, K, 2) 
  dataset_copy         =  list(folds)
  #Testing Using Fold 0 as Test Data
  test_fold            =  dataset_copy.pop(0)  
  dataset_without_fold =  dataset_copy         
  dataset_without_fold =  np.array(dataset_without_fold).reshape((-1, data.shape[1]))
  X                    = dataset_without_fold[:, bestModel["features"] ]
  y = dataset_without_fold[:, -1].reshape(-1)
  test_data            = test_fold[:, bestModel["features"] ]
  test_true_label      = test_fold[:, -1].reshape(-1)
  #Average Data Containers
  all_test_auc = np.array([])
  #Iterate all K's using the Selected Features and Fold 0 as Test Data
  for k in k_range:
    clf            = KNeighborsClassifier(k)
    train_acc      = clf.fit(X, y).score(X, y) 
    predictions    = clf.predict(test_data)
    predictions_prob              = clf.predict_proba(test_data)[:, 1] #Probability of being 1
    fpr, tpr, roc_thresholds      = roc_curve( test_true_label, predictions_prob)
    roc_auc                       = auc(fpr, tpr)
    test_acc       = accuracy_score(test_true_label, predictions.round())
    all_test_auc  = np.append(all_test_auc , roc_auc)
  k_ax.plot( k_range , all_test_auc)
  k_fig.legend(('Best_Features_Model'), loc='upper right')
  k_ax.set_ylabel('AUC')
  k_ax.set_xlabel('K Value')
#   k_ax.set_xlim(0, 1)
#   k_ax.set_ylim(0, 1)
  k_ax.set_title('Possible K´s with selected features: {}  Final K: {} AUC: {:4f}'.format(bestModel["features"], bestModel["k"], bestModel["auc_score"] ))

"""# Main"""

filename               = sys.argv[1]                 #Import data filename

data                   = loadDataset(filename)

data                   = OrderByVariance(data)

folds, n_rows_per_fold = EqualyDistributePositiveAndNegativeAndSplit(data)

bestModel, BestModelTable, f_indexes   = CrossValidation(folds, data, K)

PlotCurves( folds, bestModel, data )            #Ploting

PlotKMapMetrics(folds, bestModel, BestModelTable, f_indexes , data)

print("\nTable of Posible Values Saved at {}\n\nBest_Model\tK_Value\t{}\tSelected_Features\t{}\tAUC_Score\t{}".format(
  PossibleModelsCSV, bestModel["k"], bestModel["features"], bestModel["auc_score"]))
plt.show()

# df = pd.read_csv(PossibleModelsCSV, sep='\t', index_col=0)
# df